{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import TrainerCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast\n",
    "\n",
    "# kogpt 모델 로드\n",
    "model_name = 'skt/kogpt2-base-v2' \n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name, bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
    "    pad_token='</s>')\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "# data.txt(텍스트 데이터셋) 파일 로드\n",
    "with open(\"data.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = f.read()\n",
    "\n",
    "texts = [line.strip() for line in raw_data.split(\"\\n\") if line.strip()]\n",
    "\n",
    "\n",
    "train_size = int(0.85 * len(texts))\n",
    "train_texts = texts[:train_size]\n",
    "test_texts = texts[train_size:]\n",
    "\n",
    "train_dataset = Dataset.from_dict({'text': train_texts})\n",
    "test_dataset = Dataset.from_dict({'text': test_texts})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#토크나이징\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(examples[\"text\"], padding='max_length', max_length=512, truncation=True)\n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 파인튜닝1 \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=2,\n",
    "    evaluation_strategy=\"no\", \n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from google.colab import files\n",
    "from datasets import Dataset\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "# qadata.txt(질의응답 json형태 데이터셋) 파일 로드\n",
    "with open(\"qadata.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    qa_data = json.load(f)\n",
    "\n",
    "qa_dataset = Dataset.from_dict(qa_data)\n",
    "\n",
    "train_testsplit = qa_dataset.train_test_split(test_size=0.15)\n",
    "train_dataset = train_testsplit['train']\n",
    "test_dataset = train_testsplit['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#토크나이징\n",
    "def preprocess_function(examples):\n",
    "    inputs = [q + tokenizer.eos_token + a for q, a in zip(examples['question'], examples['answer'])]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#파인튜닝2\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    evaluation_strategy=\"epoch\", \n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,  \n",
    "    eval_dataset=tokenized_test_dataset,   \n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 테스트 예시\n",
    "test_questions = [\n",
    "        \"의무유급제도란 무엇인가요?\",\n",
    "        \"제적이란 무엇인가요?\",\n",
    "        \"유급학기의 계절수업 성적은 어떻게 되나요?\",\n",
    "        \"의무유급 학기의 성적은 어떻게 되나요?\",\n",
    "        \"수료/수여대상자가 의무유급대상이 될 경우 어떻게 되나요?\",\n",
    "        \"성적은 어떻게 평가되나요?\",\n",
    "        \"매 학기에 정기시험은 몇회 있나요?\",\n",
    "        \"A등급의 비율은 얼마로 제한되나요?\",\n",
    "        \"상대평가 대상에서 제외되는 대상은 누구인가요?\",\n",
    "        \"성적 확인이 제한되는 경우는 어떤 것이 있나요?\",\n",
    "        \"학사경고 사항은 어디에 기재되나요?\",\n",
    "        \"학사경고자는 어떤 조치가 따르나요?\",\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    inputs = tokenizer.encode(question + tokenizer.eos_token, return_tensors='pt').to(model.device)\n",
    "    outputs = model.generate(inputs, max_length=512, do_sample=True, top_k=50)\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    answer = generated_text[len(question):].strip()\n",
    "    print(f\"질문: {question}\")\n",
    "    print(f\"답변: {answer}\")\n",
    "    print(\"-\" * 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# loss 값을 저장할 클래스\n",
    "class LossRecorder(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "        self.train_epochs = []\n",
    "        self.eval_losses = []\n",
    "        self.eval_epochs = []\n",
    "    \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            if 'loss' in logs:\n",
    "                self.train_losses.append(logs['loss'])\n",
    "                self.train_epochs.append(state.epoch)\n",
    "            if 'eval_loss' in logs:\n",
    "                self.eval_losses.append(logs['eval_loss'])\n",
    "                self.eval_epochs.append(state.epoch)\n",
    "\n",
    "loss_recorder = LossRecorder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 추가학습\n",
    "training_args.num_train_epochs = 10\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    callbacks=[loss_recorder],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 손실 값 가져오기\n",
    "train_epochs = loss_recorder.train_epochs\n",
    "train_losses = loss_recorder.train_losses\n",
    "eval_epochs = loss_recorder.eval_epochs\n",
    "eval_losses = loss_recorder.eval_losses\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_epochs, train_losses, label='Training Loss')\n",
    "plt.plot(eval_epochs, eval_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 베이스라인 모델 로드\n",
    "baseline_model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "for question in test_questions:\n",
    "    inputs = tokenizer.encode(question + tokenizer.eos_token, return_tensors='pt')\n",
    "    outputs = baseline_model.generate(inputs, max_length=50, do_sample=True, top_k=50)\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    answer = generated_text[len(question):].strip()\n",
    "    print(f\"[베이스라인] 질문: {question}\")\n",
    "    print(f\"[베이스라인] 답변: {answer[len(question):]}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
